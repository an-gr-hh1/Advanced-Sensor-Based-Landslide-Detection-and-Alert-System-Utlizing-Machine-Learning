{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qD2GrJYu8tkW",
        "outputId": "46a35b7f-2b2a-4abe-caad-c881e7d88a46"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import (confusion_matrix, roc_curve, auc, accuracy_score,\n",
        "                             f1_score, log_loss, recall_score)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
        "                              ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier)\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as BayesianLDA, QuadraticDiscriminantAnalysis as BayesianQDA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# For CNN model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define hyperparameters for LightGBM\n",
        "lgb_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"n_estimators\": 1000,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"num_leaves\": 31,\n",
        "    \"max_depth\": -1,\n",
        "    \"reg_alpha\": 0.1,\n",
        "    \"reg_lambda\": 0.1,\n",
        "    \"random_state\": 5,\n",
        "}\n",
        "\n",
        "# Data preparation function\n",
        "def prepare_data(input_file, show_info=True):\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Fix types (all columns are converted to numeric; adjust if needed)\n",
        "    for col in df.columns:\n",
        "        if col in [\"LandCover\", \"Geology\", \"UID\"]:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        else:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # One-hot encode categorical columns\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    encoded_columns = encoder.fit_transform(df[[\"Geology\", \"LandCover\"]])\n",
        "    encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out([\"Geology\", \"LandCover\"]))\n",
        "    df = pd.concat([df.drop([\"Geology\", \"LandCover\"], axis=1), encoded_df], axis=1)\n",
        "\n",
        "    # Separate target variable and features\n",
        "    X = df.drop(columns='LS')\n",
        "    y = df['LS'].astype('category').cat.codes\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Stratified train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=0.3, random_state=5, shuffle=True, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, df\n",
        "\n",
        "# Load and prepare data\n",
        "input_file = \"Landslides.csv\"\n",
        "X_train, X_test, y_train, y_test, df = prepare_data(input_file)\n",
        "\n",
        "# Model Definitions (added NaiveBayes)\n",
        "models = {\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
        "    \"MultiLayerPerceptronClassifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=600, batch_size=32, alpha=0.05, learning_rate_init=0.001, random_state=0),\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
        "    \"LogisticClassifier\": LogisticRegression(max_iter=1000),\n",
        "    \"LightGBMClassifier\": lgb.LGBMClassifier(**lgb_params),\n",
        "    \"SVMClassifier\": SVC(probability=True, random_state=0),\n",
        "    \"NaiveBayesClassifier\": GaussianNB()\n",
        "}\n",
        "\n",
        "# Evaluate each non-CNN model and store results\n",
        "results = []\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Use predict_proba if available\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_probs = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_probs = y_pred\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    mean_cross_entropy = log_loss(y_test, y_probs) if hasattr(model, \"predict_proba\") else np.nan\n",
        "    recall_val = recall_score(y_test, y_pred)\n",
        "\n",
        "    # Compute AUC if probabilities are available\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "        auc_val = auc(fpr, tpr)\n",
        "    else:\n",
        "        auc_val = np.nan\n",
        "\n",
        "    # Compute specificity from confusion matrix\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    specificity_val = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Mean Cross Entropy\": mean_cross_entropy,\n",
        "        \"F1 Score\": f1,\n",
        "        \"Recall\": recall_val,\n",
        "        \"AUC\": auc_val,\n",
        "        \"Specificity\": specificity_val\n",
        "    })\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Mean Cross Entropy: {mean_cross_entropy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {recall_val:.4f}\")\n",
        "    print(f\"AUC: {auc_val:.4f}\")\n",
        "    print(f\"Specificity: {specificity_val:.4f}\\n\")\n",
        "\n",
        "# Convert results to a DataFrame for comparison\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(results_df.sort_values(by=\"Accuracy\", ascending=False))\n",
        "\n",
        "# -------------------- CNN MODEL --------------------\n",
        "\n",
        "# Reshape data for CNN (adding a channel dimension)\n",
        "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Define CNN model architecture\n",
        "def create_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "cnn_model = create_cnn_model((X_train.shape[1], 1))\n",
        "# Train the CNN (using validation split from training data)\n",
        "cnn_model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
        "\n",
        "# Predict using CNN\n",
        "y_pred_cnn_prob = cnn_model.predict(X_test_cnn).ravel()\n",
        "y_pred_cnn = (y_pred_cnn_prob > 0.5).astype(int)\n",
        "\n",
        "# Compute evaluation metrics for CNN\n",
        "accuracy = accuracy_score(y_test, y_pred_cnn)\n",
        "f1 = f1_score(y_test, y_pred_cnn)\n",
        "mean_cross_entropy = log_loss(y_test, y_pred_cnn_prob)\n",
        "recall_val = recall_score(y_test, y_pred_cnn)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_cnn_prob)\n",
        "auc_val = auc(fpr, tpr)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_cnn).ravel()\n",
        "specificity_val = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
        "\n",
        "results.append({\n",
        "    \"Model\": \"CNNClassifier\",\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Mean Cross Entropy\": mean_cross_entropy,\n",
        "    \"F1 Score\": f1,\n",
        "    \"Recall\": recall_val,\n",
        "    \"AUC\": auc_val,\n",
        "    \"Specificity\": specificity_val\n",
        "})\n",
        "\n",
        "print(f\"Model: CNNClassifier\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Mean Cross Entropy: {mean_cross_entropy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Recall: {recall_val:.4f}\")\n",
        "print(f\"AUC: {auc_val:.4f}\")\n",
        "print(f\"Specificity: {specificity_val:.4f}\\n\")\n",
        "\n",
        "# Update results DataFrame to include CNN\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nUpdated Model Comparison:\")\n",
        "print(results_df.sort_values(by=\"Accuracy\", ascending=False))\n",
        "\n",
        "# Plot ROC curve for all models (only models with probability estimates)\n",
        "plt.figure(figsize=(10, 8))\n",
        "for model_name, model in models.items():\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_probs = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "        plt.plot(fpr, tpr, label=f\"{model_name}\")\n",
        "\n",
        "# Add CNN ROC curve\n",
        "plt.plot(fpr, tpr, label=\"CNNClassifier\")\n",
        "\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"roc_plot.png\", dpi=300)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
